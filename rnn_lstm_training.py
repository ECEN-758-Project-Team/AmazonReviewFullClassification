# -*- coding: utf-8 -*-
"""RNN_LSTM_Training

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c7VopmTlUlozWBwphroo7DnxuQMK-tZg
"""

"""Combined Amazon Review EDA and RNN/LSTM Training

This script combines data loading, EDA, and model training for Amazon reviews.
"""

import os, tarfile, pandas as pd, gdown, re, warnings
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from transformers import AutoTokenizer

warnings.filterwarnings('ignore')

# DATA DOWNLOAD AND LOADING

print("="*80)
print("AMAZON REVIEW SENTIMENT ANALYSIS - DATA PREPARATION")
print("="*80)

# Find the user's downloads folder
downloads_folder = Path.home() / "Downloads"
ROOT = downloads_folder / "Amazon Data"
os.makedirs(ROOT, exist_ok=True)
print(f"\nDataset will be stored at: {ROOT}")

# Download the official AmazonReviewFull tar.gz from Google Drive
url_id = "0Bz8a_Dbh9QhbZVhsUnRWRDhETzA"
tar_path = os.path.join(ROOT, "amazon_review_full_csv.tar.gz")

if not os.path.exists(tar_path):
    print("Downloading dataset tar...")
    gdown.download(id=url_id, output=tar_path, quiet=False, resume=True)

# Extract zip file if needed
extract_dir = os.path.join(ROOT, "amazon_review_full_csv")
train_csv = os.path.join(extract_dir, "train.csv")
test_csv = os.path.join(extract_dir, "test.csv")

if not os.path.exists(train_csv):
    print("Extracting tar...")
    with tarfile.open(tar_path, "r:gz") as tar:
        tar.extractall(path=ROOT)

print(f"\nTrain file exists: {os.path.exists(train_csv)}")
print(f"Test file exists: {os.path.exists(test_csv)}")

# DATA LOADING

# Ask user for number of rows to load
try:
    num_rows = int(input("\nEnter how many rows to load from train.csv (max 3000000): "))
except ValueError:
    print("Invalid input. Defaulting to 100000 rows")
    num_rows = 100000

try:
    test_rows = int(input("Enter how many rows to load from test.csv (max 650000): "))
except ValueError:
    print("Invalid input. Defaulting to 15000 rows")
    test_rows = 15000

# Load data
cols = ["label", "title", "review"]
df = pd.read_csv(train_csv, header=None, names=cols, nrows=num_rows, on_bad_lines='skip')
test_df = pd.read_csv(test_csv, header=None, names=cols, nrows=test_rows, on_bad_lines='skip')

# Drop rows with missing reviews
df = df.dropna(subset=["review"])
test_df = test_df.dropna(subset=["review"])

print(f"\nLoaded {len(df)} rows from train.csv")
print(f"Loaded {len(test_df)} rows from test.csv")

# DATA CLEANING

print("\n" + "="*80)
print("DATA CLEANING")
print("="*80)

def clean_text(text):
    """Clean and preprocess text"""
    text = str(text)
    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags
    text = re.sub(r'http\S+', '', text)  # Remove URLs
    text = text.lower()
    text = re.sub(r'[^a-z0-9\s]', ' ', text)  # Keep only letters, numbers, spaces
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra whitespace
    return text

# Combine title + review
df['text'] = df['title'].fillna('') + ' ' + df['review'].fillna('')
test_df['text'] = test_df['title'].fillna('') + ' ' + test_df['review'].fillna('')

# Clean text
df['text'] = df['text'].apply(clean_text)
test_df['text'] = test_df['text'].apply(clean_text)

print("Text cleaning completed")

# TRAIN/VALIDATION SPLIT

print("\n" + "="*80)
print("TRAIN/VALIDATION SPLIT")
print("="*80)

# Ask for validation fraction
try:
    val_frac = float(input("\nEnter validation fraction (0-1, e.g., 0.2 for 20%): "))
    if not (0 < val_frac < 1):
        raise ValueError
except ValueError:
    print("Invalid input, defaulting to 0.15")
    val_frac = 0.15

# Labels are 1-5, we'll convert to 0-4 for PyTorch
df['label'] = df['label'].astype(int)
test_df['label'] = test_df['label'].astype(int)

# Split training data into train and validation
X_temp = df['text'].values
y_temp = df['label'].values

X_train_text, X_val_text, y_train_raw, y_val_raw = train_test_split(
    X_temp, y_temp, test_size=val_frac, random_state=42, stratify=y_temp
)

X_test_text = test_df['text'].values
y_test_raw = test_df['label'].values

print(f"\nTrain samples: {len(X_train_text)}")
print(f"Validation samples: {len(X_val_text)}")
print(f"Test samples: {len(X_test_text)}")
print(f"Label range: {y_temp.min()}-{y_temp.max()}")

# EXPLORATORY DATA ANALYSIS

print("\n" + "="*80)
print("EXPLORATORY DATA ANALYSIS")
print("="*80)

sns.set_style("whitegrid")

# 1. Rating Distribution
print("\n1. RATING DISTRIBUTION ANALYSIS")
print("-"*80)

print("\nTraining Set:")
train_rating_counts = pd.Series(y_train_raw).value_counts().sort_index()
for rating, count in train_rating_counts.items():
    percentage = (count / len(y_train_raw)) * 100
    bar = "█" * int(percentage / 2)
    print(f"  Rating {rating}: {count:>6,} reviews ({percentage:>5.2f}%) {bar}")

print("\nValidation Set:")
val_rating_counts = pd.Series(y_val_raw).value_counts().sort_index()
for rating, count in val_rating_counts.items():
    percentage = (count / len(y_val_raw)) * 100
    bar = "█" * int(percentage / 2)
    print(f"  Rating {rating}: {count:>6,} reviews ({percentage:>5.2f}%) {bar}")

print("\nTest Set:")
test_rating_counts = pd.Series(y_test_raw).value_counts().sort_index()
for rating, count in test_rating_counts.items():
    percentage = (count / len(y_test_raw)) * 100
    bar = "█" * int(percentage / 2)
    print(f"  Rating {rating}: {count:>6,} reviews ({percentage:>5.2f}%) {bar}")

# 2. Text Length Statistics
print("\n2. TEXT LENGTH STATISTICS")
print("-"*80)

train_text_lengths = [len(text.split()) for text in X_train_text]
test_text_lengths = [len(text.split()) for text in X_test_text]

print("\nTraining Data:")
print(f"  Mean length: {np.mean(train_text_lengths):.2f} words")
print(f"  Median length: {np.median(train_text_lengths):.2f} words")
print(f"  Min length: {np.min(train_text_lengths)} words")
print(f"  Max length: {np.max(train_text_lengths)} words")
print(f"  Std deviation: {np.std(train_text_lengths):.2f} words")

# 3. Most Common Words
print("\n3. MOST COMMON WORDS ANALYSIS")
print("-"*80)

all_train_words = []
for text in X_train_text:
    words = text.split()
    filtered_words = [word for word in words if word not in ENGLISH_STOP_WORDS and len(word) > 2]
    all_train_words.extend(filtered_words)

word_freq = Counter(all_train_words)
most_common_words = word_freq.most_common(20)

print("\nTop 20 Most Common Words:")
for idx, (word, count) in enumerate(most_common_words, 1):
    print(f"  {idx:>2}. '{word:<15}': {count:>7,} occurrences")

# TOKENIZATION FOR PYTORCH MODELS

print("\n" + "="*80)
print("TOKENIZATION FOR PYTORCH MODELS")
print("="*80)

# Use BERT tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
max_words = 200

print(f"\nTokenizing with max length: {max_words}")

# Tokenize all splits
encodings_train = tokenizer(
    list(X_train_text),
    padding='max_length',
    truncation=True,
    max_length=max_words,
    return_tensors='np'
)

encodings_val = tokenizer(
    list(X_val_text),
    padding='max_length',
    truncation=True,
    max_length=max_words,
    return_tensors='np'
)

encodings_test = tokenizer(
    list(X_test_text),
    padding='max_length',
    truncation=True,
    max_length=max_words,
    return_tensors='np'
)

X_train = encodings_train["input_ids"]
X_val = encodings_val["input_ids"]
X_test = encodings_test["input_ids"]

# Convert labels from 1-5 to 0-4 for PyTorch
y_train = y_train_raw - 1
y_val = y_val_raw - 1
y_test = y_test_raw - 1

print(f"\nData shapes:")
print(f"  X_train: {X_train.shape}, y_train: {y_train.shape}")
print(f"  X_val: {X_val.shape}, y_val: {y_val.shape}")
print(f"  X_test: {X_test.shape}, y_test: {y_test.shape}")
print(f"\nLabel distribution (0-4): {np.bincount(y_train)}")

# Convert to PyTorch tensors
X_train_t = torch.tensor(X_train, dtype=torch.long)
y_train_t = torch.tensor(y_train, dtype=torch.long)
X_val_t = torch.tensor(X_val, dtype=torch.long)
y_val_t = torch.tensor(y_val, dtype=torch.long)
X_test_t = torch.tensor(X_test, dtype=torch.long)
y_test_t = torch.tensor(y_test, dtype=torch.long)

# Create data loaders
batch_size = 128
train_loader = DataLoader(TensorDataset(X_train_t, y_train_t),
                         batch_size=batch_size, shuffle=True)
val_loader = DataLoader(TensorDataset(X_val_t, y_val_t),
                       batch_size=batch_size, shuffle=False)
test_loader = DataLoader(TensorDataset(X_test_t, y_test_t),
                        batch_size=batch_size, shuffle=False)

print(f"DataLoaders created with batch_size={batch_size}")

#=============================================================================
# MODEL DEFINITIONS
#=============================================================================

print("\n" + "="*80)
print("MODEL DEFINITIONS")
print("="*80)

class RNNClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256,
                 output_dim=5, num_layers=2, dropout=0.3):
        super(RNNClassifier, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)

        self.rnn = nn.RNN(
            embedding_dim,
            hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=True,
            nonlinearity='tanh'
        )

        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_dim * 2, output_dim)

    def forward(self, x):
        embedded = self.embedding(x)
        rnn_out, hidden = self.rnn(embedded)
        last_hidden = rnn_out[:, -1, :]
        dropped = self.dropout(last_hidden)
        logits = self.fc(dropped)
        return logits


class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256,
                 output_dim=5, num_layers=2, dropout=0.3):
        super(LSTMClassifier, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)

        self.lstm = nn.LSTM(
            embedding_dim,
            hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=True
        )

        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_dim * 2, output_dim)

    def forward(self, x):
        embedded = self.embedding(x)
        lstm_out, (hidden, cell) = self.lstm(embedded)
        last_hidden = lstm_out[:, -1, :]
        dropped = self.dropout(last_hidden)
        logits = self.fc(dropped)
        return logits


#=============================================================================
# TRAINING FUNCTIONS
#=============================================================================

def train_epoch(model, loader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    all_preds, all_labels = [], []

    for X_batch, y_batch in loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)

        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        total_loss += loss.item()
        preds = torch.argmax(outputs, dim=1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(y_batch.cpu().numpy())

    avg_loss = total_loss / len(loader)
    accuracy = accuracy_score(all_labels, all_preds)
    return avg_loss, accuracy


def evaluate(model, loader, criterion, device):
    model.eval()
    total_loss = 0
    all_preds, all_labels = [], []

    with torch.no_grad():
        for X_batch, y_batch in loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            total_loss += loss.item()

            preds = torch.argmax(outputs, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(y_batch.cpu().numpy())

    avg_loss = total_loss / len(loader)
    accuracy = accuracy_score(all_labels, all_preds)
    return avg_loss, accuracy, all_preds, all_labels


#=============================================================================
# RNN MODEL TRAINING
#=============================================================================

print("\n" + "="*80)
print("RNN MODEL TRAINING")
print("="*80)

vocab_size = len(tokenizer.get_vocab())
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"\nUsing device: {device}")
print(f"Vocabulary size: {vocab_size}")

rnn_model = RNNClassifier(
    vocab_size=vocab_size,
    embedding_dim=128,
    hidden_dim=256,
    output_dim=5,
    num_layers=2,
    dropout=0.3
)
rnn_model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(rnn_model.parameters(), lr=0.001)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='max', factor=0.5, patience=2
)

rnn_history = {
    'train_loss': [],
    'train_acc': [],
    'val_loss': [],
    'val_acc': []
}

epochs = 20
best_val_acc = 0
patience_counter = 0
max_patience = 5

print(f"\nTraining RNN for up to {epochs} epochs...")

for epoch in range(epochs):
    train_loss, train_acc = train_epoch(rnn_model, train_loader, criterion, optimizer, device)
    val_loss, val_acc, _, _ = evaluate(rnn_model, val_loader, criterion, device)

    print(f"Epoch {epoch+1}/{epochs}")
    print(f"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}")
    print(f"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}")

    rnn_history['train_loss'].append(train_loss)
    rnn_history['train_acc'].append(train_acc)
    rnn_history['val_loss'].append(val_loss)
    rnn_history['val_acc'].append(val_acc)

    scheduler.step(val_acc)

    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(rnn_model.state_dict(), os.path.join(ROOT, 'best_rnn_model.pt'))
        patience_counter = 0
    else:
        patience_counter += 1
        if patience_counter >= max_patience:
            print(f"Early stopping triggered after {epoch+1} epochs")
            break
    print()

# Evaluate RNN on test set
print("\n" + "="*80)
print("RNN TEST RESULTS")
print("="*80)

rnn_model.load_state_dict(torch.load(os.path.join(ROOT, 'best_rnn_model.pt')))
test_loss, test_acc, test_preds, test_labels = evaluate(rnn_model, test_loader, criterion, device)

print(f"\nTest Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_acc:.4f}")
print("\nClassification Report:")
print(classification_report(test_labels, test_preds,
                          target_names=[f'Rating {i+1}' for i in range(5)]))

rnn_test_acc = test_acc
rnn_test_loss = test_loss

#=============================================================================
# LSTM MODEL TRAINING
#=============================================================================

print("\n" + "="*80)
print("LSTM MODEL TRAINING")
print("="*80)

lstm_model = LSTMClassifier(
    vocab_size=vocab_size,
    embedding_dim=128,
    hidden_dim=256,
    output_dim=5,
    num_layers=2,
    dropout=0.3
)
lstm_model.to(device)

optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='max', factor=0.5, patience=2
)

lstm_history = {
    'train_loss': [],
    'train_acc': [],
    'val_loss': [],
    'val_acc': []
}

best_val_acc = 0
patience_counter = 0

print(f"\nTraining LSTM for up to {epochs} epochs...")

for epoch in range(epochs):
    train_loss, train_acc = train_epoch(lstm_model, train_loader, criterion, optimizer, device)
    val_loss, val_acc, _, _ = evaluate(lstm_model, val_loader, criterion, device)

    print(f"Epoch {epoch+1}/{epochs}")
    print(f"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}")
    print(f"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}")

    lstm_history['train_loss'].append(train_loss)
    lstm_history['train_acc'].append(train_acc)
    lstm_history['val_loss'].append(val_loss)
    lstm_history['val_acc'].append(val_acc)

    scheduler.step(val_acc)

    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(lstm_model.state_dict(), os.path.join(ROOT, 'best_lstm_model.pt'))
        patience_counter = 0
    else:
        patience_counter += 1
        if patience_counter >= max_patience:
            print(f"Early stopping triggered after {epoch+1} epochs")
            break
    print()

# Evaluate LSTM on test set
print("\n" + "="*80)
print("LSTM TEST RESULTS")
print("="*80)

lstm_model.load_state_dict(torch.load(os.path.join(ROOT, 'best_lstm_model.pt')))
test_loss, test_acc, test_preds_lstm, test_labels_lstm = evaluate(lstm_model, test_loader, criterion, device)

print(f"\nTest Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_acc:.4f}")
print("\nClassification Report:")
print(classification_report(test_labels_lstm, test_preds_lstm,
                          target_names=[f'Rating {i+1}' for i in range(5)]))

lstm_test_acc = test_acc
lstm_test_loss = test_loss

# COMPARISON TABLE

print("\n" + "="*80)
print("COMPARATIVE PERFORMANCE METRICS")
print("="*80)

# Calculate metrics for both models
rnn_precision, rnn_recall, rnn_f1, _ = precision_recall_fscore_support(
    test_labels, test_preds, average='macro'
)
lstm_precision, lstm_recall, lstm_f1, _ = precision_recall_fscore_support(
    test_labels_lstm, test_preds_lstm, average='macro'
)

comparison_metrics = {
    'Model': ['RNN', 'LSTM'],
    'Test Acc': [f'{rnn_test_acc:.4f}', f'{lstm_test_acc:.4f}'],
    'Test Loss': [f'{rnn_test_loss:.4f}', f'{lstm_test_loss:.4f}'],
    'Val Acc': [f'{max(rnn_history["val_acc"]):.4f}', f'{max(lstm_history["val_acc"]):.4f}'],
    'Avg Precision': [f'{rnn_precision:.4f}', f'{lstm_precision:.4f}'],
    'Avg Recall': [f'{rnn_recall:.4f}', f'{lstm_recall:.4f}'],
    'Avg F1': [f'{rnn_f1:.4f}', f'{lstm_f1:.4f}'],
    'Parameters (M)': [f'{sum(p.numel() for p in rnn_model.parameters())/1e6:.2f}',
                       f'{sum(p.numel() for p in lstm_model.parameters())/1e6:.2f}'],
    'Epochs': [len(rnn_history['train_loss']), len(lstm_history['train_loss'])]
}

df_comparison = pd.DataFrame(comparison_metrics)
print("\n" + df_comparison.to_string(index=False))

# Save comparison
df_comparison.to_csv(os.path.join(ROOT, 'model_comparison.csv'), index=False)

print("\n" + "="*80)
print("TRAINING COMPLETE")
print("="*80)
print(f"\nAll results saved to: {ROOT}")
print("Files saved:")
print("  - best_rnn_model.pt")
print("  - best_lstm_model.pt")
print("  - model_comparison.csv")